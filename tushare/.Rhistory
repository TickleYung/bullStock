library(rstanarm)
library(rstanarm)
install.packages("rstanarm")
install.packages("ggplot2")
install.packages("loo")
install.packages("caret")
install.packages("pROC")
knitr::opts_chunk$set(echo = TRUE)
setwd("D:\\2.Code\\github\\Statistics\\Bayesian Logistic Regression with rstanarm")
# file preview shows a header row
diabetes <- read.csv("diabetes.csv", header = TRUE)
# first look at the data set using summary() and str() to understand what type of data are you working with
dim(diabetes)
summary(diabetes)
str(diabetes)
diabetes$Outcome <- factor(diabetes$Outcome)
# removing those observation rows with 0 in any of the variables
for (i in 2:6) {
diabetes <- diabetes[-which(diabetes[, i] == 0), ]
}
# scale the covariates for easier comparison of coefficient posteriors
for (i in 1:8) {
diabetes[i] <- scale(diabetes[i])
}
# modify the data column names slightly for easier typing
names(diabetes)[7] <- "dpf"
names(diabetes) <- tolower(names(diabetes))
n=dim(diabetes)[1]
p=dim(diabetes)[2]
str(diabetes)
print(paste0("number of observations = ", n))
print(paste0("number of predictors = ", p))
# preparing the inputs
x <- model.matrix(outcome ~ . - 1, data = diabetes)
y <- diabetes$outcome
library(rstanarm)
library(rstanarm)
install.packages("library(rstanarm)")
install.packages("rstanarm")
library(rstanarm)
install.packages("rlang")
library(rstanarm)
library(rstanarm)
t_prior <- student_t(df = 7, location = 0, scale = 2.5)
post1 <- stan_glm(outcome ~ ., data = diabetes,
family = binomial(link = "logit"),
prior = t_prior, prior_intercept = t_prior,
seed = 1)
library(ggplot2)
pplot<-plot(post1, "areas", prob = 0.95, prob_outer = 1)
pplot+ geom_vline(xintercept = 0)
library(ggplot2)
pplot<-plot(post1, "areas", prob = 0.95, prob_outer = 1)
pplot+ geom_vline(xintercept = 0)
round(coef(post1), 2)
round(posterior_interval(post1, prob = 0.9), 2)
library(loo)
(loo1 <- loo(post1))
# Predicted probabilities
linpred <- posterior_linpred(post1)
preds <- posterior_linpred(post1, transform=TRUE)
pred <- colMeans(preds)
pr <- as.integer(pred >= 0.5)
library(caret)
# confusion matrix
confusionMatrix(pr, y)[2:3]
y
library(caret)
confusionMatrix(pr, y)[2:3]
pr
y
round(coef(post1), 2)
round(posterior_interval(post1, prob = 0.9), 2)
library(loo)
(loo1 <- loo(post1))
# Predicted probabilities
linpred <- posterior_linpred(post1)
preds <- posterior_linpred(post1, transform=TRUE)
pred <- colMeans(preds)
pr <- as.integer(pred >= 0.5)
library(caret)
# confusion matrix
pr <- factor(pr)
confusionMatrix(pr, y)[2:3]
install.packages("caret")
install.packages("caret")
library(caret)
pr <- factor(pr)
confusionMatrix(pr, y)[2:3]
install.packages("e1071")
library(caret)
confusionMatrix(pr, y)[2:3]
round(coef(post1), 2)
round(posterior_interval(post1, prob = 0.9), 2)
library(loo)
(loo1 <- loo(post1))
# Predicted probabilities
linpred <- posterior_linpred(post1)
preds <- posterior_linpred(post1, transform=TRUE)
pred <- colMeans(preds)
pr <- as.integer(pred >= 0.5)
library(caret)
# confusion matrix
# pr <- factor(pr)
confusionMatrix(pr, y)[2:3]
round(coef(post1), 2)
round(posterior_interval(post1, prob = 0.9), 2)
library(loo)
(loo1 <- loo(post1))
# Predicted probabilities
linpred <- posterior_linpred(post1)
preds <- posterior_linpred(post1, transform=TRUE)
pred <- colMeans(preds)
pr <- as.integer(pred >= 0.5)
library(caret)
# confusion matrix
pr <- factor(pr)
confusionMatrix(pr, y)[2:3]
# posterior classification accuracy
round(mean(xor(pr,as.integer(y))),3)
# posterior balanced classification accuracy
round((mean(xor(pr[y==0]>0.5,as.integer(y[y==0])))+mean(xor(pr[y==1]>0.5,as.integer(y[y==1]))))/2,3)
# PSIS-LOO weights
log_lik=log_lik(post1, parameter_name = "log_lik")
psis=psislw(-log_lik)
#plot(psis$pareto_k)
#plot(psis$lw_smooth[,1],linpred[,1])
# LOO predictive probabilities
ploo=colSums(preds*exp(psis$lw_smooth))
# LOO classification accuracy
round(mean(xor(ploo>0.5,as.integer(y))),3)
# LOO balanced classification accuracy
round((mean(xor(ploo[y==0]>0.5,as.integer(y[y==0])))+mean(xor(ploo[y==1]>0.5,as.integer(y[y==1]))))/2,2)
plot(pred,ploo)
# compute AUCs
library(pROC)
plot.roc(y,pred,percent=TRUE,col="#1c61b6",  print.auc=TRUE)
plot.roc(y,ploo,percent=TRUE,col="#008600",  print.auc=TRUE, print.auc.y=40, add=TRUE)
legend("bottomright", legend=c("Posterior ROC", "LOO ROC"), col=c("#1c61b6", "#008600"), lwd=2)
p0 <- 2 # prior guess for the number of relevant variables
tau0 <- p0/(p-p0) * 1/sqrt(n)
hs_prior <- hs(df=1, global_df=1, global_scale=tau0)
t_prior <- student_t(df = 7, location = 0, scale = 2.5)
post2 <- stan_glm(outcome ~ ., data = diabetes,
family = binomial(link = "logit"),
prior = hs_prior, prior_intercept = t_prior,
seed = 1, adapt_delta = 0.999)
pplot<-plot(post2, "areas", prob = 0.95, prob_outer = 1)
pplot+ geom_vline(xintercept = 0)
round(coef(post2), 2)
round(posterior_interval(post2, prob = 0.9), 2)
(loo2 <- loo(post2))
compare(loo1,loo2)
save.image("all.Rdata")
install.packages('lars')
install.packages('glmnet')
library(lars)
# https://cran.r-project.org/web/packages/lars/lars.pdf
library(glmnet)
data(diabetes)
diabetes
x<-diabetes$x
y<-diabetes$y
x
y
boxplot(y)
oldpar=par()
par(mfrow=c(2,5))
for(i in 1:10){
plot(x[,i], y)
abline(lm(y~x[,i]))
}
model_ols <- lm(y ~ x)
summary(model_ols)
model_lasso <- glmnet(x, y, family="gaussian", nlambda=50, alpha=1)
knitr::opts_chunk$set(echo = TRUE)
install.packages('lars')
install.packages('glmnet')
library(lars)
# https://cran.r-project.org/web/packages/lars/lars.pdf
library(glmnet)
install.packages("glmnet")
install.packages("lars")
knitr::opts_chunk$set(echo = TRUE)
install.packages('lars')
install.packages('glmnet')
library(lars)
# https://cran.r-project.org/web/packages/lars/lars.pdf
library(glmnet)
library(lars)
library(glmnet)
install.packages('glmnet')
library(glmnet)
# install.packages('lars')
# install.packages('glmnet')
library(lars)
# https://cran.r-project.org/web/packages/lars/lars.pdf
library(glmnet)
data(diabetes)
x<-diabetes$x
y<-diabetes$y
boxplot(y)
#summary(x2)
oldpar=par()
par(mfrow=c(2,5))
for(i in 1:10){
plot(x[,i], y)
abline(lm(y~x[,i]))
}
model_ols <- lm(y ~ x)
summary(model_ols)
model_lasso <- glmnet(x, y, family="gaussian", nlambda=50, alpha=1)
print(model_lasso)
coef(model_lasso, s=c(model_lasso$lambda[29],0.23))
plot.glmnet(model_lasso, xvar = "norm", label = TRUE)
plot(model_lasso, xvar="lambda", label=TRUE)
pre <- predict(model_lasso, newx=x , s=c(model_lasso$lambda[29],0.23))
head(cbind(y,pre))
cv_fit <- cv.glmnet(x=x, y=y, alpha = 1, nlambda = 1000)
#可视化交叉验证结果
plot.cv.glmnet(cv_fit)
# 因為交叉驗證，對於每一個λ值，在紅點所示目標參量的均值左右，我們可以得到一個目標參量的置信區間兩條虛線分別指示了兩個特殊的λ值
c(cv_fit$lambda.min,cv_fit$lambda.1se)
## [1] 1.140661 8.913526
pre <-  predict(cv_fit, newx=x[1:5,], type="response", s="lambda.1se")
head(cbind(y,pre))
# 提取模型的係數(lambda.min是指在所有的λ值中，得到最小目標參量均值的那一個)
cv_fit$lambda.min
# fit
fit <- glmnet(x=x, y=y, alpha = 1, lambda=cv_fit$lambda.min)
fit$beta
# coef
coef(cv_fit$glmnet.fit, s =  min(cv_fit$lambda))
# 提取模型的係數(指在lambda.min一個方差範圍內得到最簡單模型的那一個λ值)
cv_fit$lambda.1se
# fit
fit <- glmnet(x=x, y=y, alpha = 1, lambda=cv_fit$lambda.1se)
fit$beta
# coef
coef(cv_fit$glmnet.fit, s = cv_fit$lambda.1se)
library(doParallel)
install.packages("doParallel")
# install.packages("doParallel")
library(doParallel)
# Windows System
cl <- makeCluster(6)
registerDoParallel(cl)
cvfit = cv.glmnet(x, y, family = "binomial", type.measure = "class", parallel=TRUE)
x
y
# install.packages("doParallel")
library(doParallel)
# Windows System
cl <- makeCluster(2)
registerDoParallel(cl)
cvfit = cv.glmnet(x, y, family = "binomial", type.measure = "class", parallel=TRUE)
# install.packages("doParallel")
library(doParallel)
# Windows System
cl <- makeCluster(2)
registerDoParallel(cl)
cvfit = cv.glmnet(x, y, family = "binomial", type.measure = "class", parallel=TRUE)
x
y
# 下面是一个例子
df=data.frame(Factor=factor(1:5), Character=c("a","a","b","b","c"),
Logical=c(T,F,T,T,T), Numeric=c(2.1,2.3,2.5,4.1,1.1))
model.matrix(~., df)
library(limma)
install.packages("limma")
library(RColorBrewer)
library(rtracklayer)
install.packages("rtracklayer")
library(glmnet)
library(survival)
library(Hmisc)
install.packages("Hmisc")
library(randomForestSRC)
install.packages("randomForestSRC")
library(mg14)
install.packages("mg14")
# install.packages("limma")
# install.packages("rtracklayer")
# install.packages("Hmisc")
# install.packages("randomForestSRC")
# install.packages("mg14")
library(limma)
install.packages("limma")
# install.packages("limma")
# install.packages("rtracklayer")
# install.packages("Hmisc")
# install.packages("randomForestSRC")
# install.packages("mg14")
library(limma)
source("https://bioconductor.org/biocLite.R")
biocLite("limma")
# source("https://bioconductor.org/biocLite.R")
# biocLite("limma")
# install.packages("rtracklayer")
# install.packages("Hmisc")
# install.packages("randomForestSRC")
# install.packages("mg14")
library(limma)
library(RColorBrewer)
library(rtracklayer)
biocLite("rtracklayer")
# source("https://bioconductor.org/biocLite.R")
# biocLite("limma")
# biocLite("rtracklayer")
# install.packages("Hmisc")
# install.packages("randomForestSRC")
# install.packages("mg14")
library(limma)
library(RColorBrewer)
library(rtracklayer)
library(glmnet)
library(survival)
library(Hmisc)
library(randomForestSRC)
library(mg14)
biocLite("mg14")
install.packages("mg14")
library(devtools); install_github("mg14/mg14")
install.packages("devtools")
library(devtools); install_github("mg14/mg14")
# source("https://bioconductor.org/biocLite.R")
# biocLite("limma")
# biocLite("rtracklayer")
# install.packages("Hmisc")
# install.packages("randomForestSRC")
# library(devtools); install_github("mg14/mg14")
library(limma)
library(RColorBrewer)
library(rtracklayer)
library(glmnet)
library(survival)
library(Hmisc)
library(randomForestSRC)
library(mg14)
set1 = c(brewer.pal(9,"Set1"), brewer.pal(8, "Dark2"))
for(j in 1:ncol(X))
X[is.na(X[,j]),j] <- mean(X[,j], na.rm=TRUE)
# install.packages("doParallel")
library(doParallel)
# Windows System
cl <- makeCluster(2)
registerDoParallel(cl)
#cvfit = cv.glmnet(x, y, family = "binomial", type.measure = "class", parallel=TRUE)
stopCluster(cl)
# Linux System
# registerDoParallel(cores=2)
# cvfit = cv.glmnet(x, y, family = "binomial", type.measure = "class", parallel=TRUE)
# stopImplicitCluster()
install.packages("tinytex")
install.packages("TeX")
d <- read.table(file='http://www.win-vector.com/dfiles/glmLoss/dGLMdat.csv',
header=T,sep=',')
d
lm(y~x1+x2,data=d)
m <- glm(y~x1+x2,data=d,family=binomial(link='logit'))
summary(m)
a <- c(1,2)
b <- c(2,4)
c <- data.frame(x=a,y=b)
c
plot(c)
a <- c(1,2,3,4)
b <- c(2,4,6,8)
c <- data.frame(x=a,y=b)
c
plot(c)
cor(c)
cor(t(c))
?dist
dist(c)
dist(t(c))
dist(c)[1,1]
dist(c)
class(dist(c))
dist(c)
dist(c, method = "euclidean")
sqrt(8)
sqrt(2^2+1^2)
data(bsstop)
x=bsstop[,5:14]
# identify multivariate outliers
x.out=pcout(x,makeplot=FALSE)
# visualize multivariate outliers in the map
op <- par(mfrow=c(1,2))
data(bss.background)
pbb(asp=1)
points(bsstop$XCOO,bsstop$YCOO,pch=16,col=x.out$wfinal01+2)
title("Outlier detection based on pcout")
legend("topleft",legend=c("potential outliers","regular observations"),pch=16,col=c(2,3))
install.packages("mvoutlier")
library(mvoutlier)
data(bsstop)
x=bsstop[,5:14]
# identify multivariate outliers
x.out=pcout(x,makeplot=FALSE)
# visualize multivariate outliers in the map
op <- par(mfrow=c(1,2))
data(bss.background)
pbb(asp=1)
points(bsstop$XCOO,bsstop$YCOO,pch=16,col=x.out$wfinal01+2)
title("Outlier detection based on pcout")
legend("topleft",legend=c("potential outliers","regular observations"),pch=16,col=c(2,3))
x
dim(x)
x.out=pcout(x,makeplot=T)
x.out$wfinal01
?pcout
pcout
x.mcd <- robustbase::covMcd(x)
pbb(asp=1)
points(bsstop$XCOO,bsstop$YCOO,pch=16,col=x.mcd$mcd.wt+2)
title("Outlier detection based on MCD")
legend("topleft",legend=c("potential outliers","regular observations"),pch=16,col=c(2,3))
par(op)
?strsplit
?sub
setwd("D:\\2.Code\\github\\bullStock\\tushare")
options(stringsAsFactors = F)
filterCode <- read.csv("filterCode.csv", header = T, colClasses = "character")
filterCode
filterCode[1:5,1:5]
filterCode <- read.csv("filterCode.csv", header = T, colClasses = "character", encoding = "utf-8")
filterCode[1:5,1:5]
?read.csv
filterCode <- read.csv("filterCode.csv", header = T, colClasses = "character", fileEncoding = "utf-8")
filterCode[1:5,1:5]
filterCode <- read.table("filterCode.csv", header = T, colClasses = "character", fileEncoding = "utf-8")
filterCode <- read.csv("filterCode.csv", header = T, colClasses = "character", fileEncoding = "utf-8")
filterCode <- read.csv("filterCode.csv", header = T, colClasses = "character")
filterCode[1:5,1:5]
filterCode <- read.csv("filterCode.csv", header = T, colClasses = "character")
filterCode[1:5,1:5]
filterCode <- read.csv("filterCode.csv", header = T, colClasses = "character", encoding = "utf-8")
filterCode[1:5,1:5]
all_basics <- read.csv("stock_basics.csv", header = T, colClasses = "character")
rownames(all_basics) <- all_basics$code
all_basics[1:5,1:5]
filterCode <- read.csv("filterCode.csv", header = T, colClasses = "character", encoding = "GBK")
all_basics[1:5,1:5]
filterCode <- read.csv("filterCode.csv", header = T, colClasses = "character", ,fileEncoding="utf8")
filterCode <- read.csv("filterCode.csv", header = T, colClasses = "character",fileEncoding="utf8")
filterCode <- read.csv("filterCode.csv", header = F, colClasses = "character",fileEncoding="utf8")
filterCode <- read.csv("filterCode.csv", header = T, colClasses = "character",fileEncoding="utf8")
filterCode <- read.csv("filterCode.csv", header = T, colClasses = "character",fileEncoding="utf8")
filterCode <- read.csv("filterCode.csv", header = T, colClasses = "character")
all_basics[1:5,1:5]
